{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "    Kmeans algorithm is an iterative algorithm that tries to partition the dataset into Kpre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the inter-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster’s centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.\n",
    "    The way kmeans algorithm works is as follows:\n",
    "    1.Specify number of clusters K.\n",
    "    2.Initialize centroids by first shuffling the dataset and then randomly selecting K data points for the centroids without replacement.\n",
    "    3.Keep iterating until there is no change to the centroids. i.e assignment of data points to clusters isn’t changing.\n",
    "    4.Compute the sum of the squared distance between data points and all centroids.\n",
    "    5.Assign each data point to the closest cluster (centroid).\n",
    "    6.Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster.\n",
    "    \n",
    "    The approach kmeans follows to solve the problem is called Expectation-Maximization. The E-step is assigning the data points to the closest cluster. The M-step is computing the centroid of each cluster. Below is a break down of how we can solve it mathematically (feel free to skip it).\n",
    "    \n",
    "    The objective function is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pic1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    where wik=1 for data point xi if it belongs to cluster k; otherwise, wik=0. Also, μk is the centroid of xi’s cluster.\n",
    "    It’s a minimization problem of two parts. We first minimize J w.r.t. wik and treat μk fixed. Then we minimize J w.r.t. μk and treat wik fixed. Technically speaking, we differentiate J w.r.t. wik first and update cluster assignments (E-step). Then we differentiate J w.r.t. μk and recompute the centroids after the cluster assignments from previous step (M-step). Therefore, E-step is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pic2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In other words, assign the data point xi to the closest cluster judged by its sum of squared distance from cluster’s centroid.\n",
    "    \n",
    "    And M-step is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pic3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Which translates to recomputing the centroid of each cluster to reflect the new assignments.\n",
    "\n",
    "    Few things to note here:\n",
    "\n",
    "    1. Since clustering algorithms including kmeans use distance-based measurements to determine the similarity between data points, it’s recommended to standardize the data to have a mean of zero and a standard deviation of one since almost always the features in any dataset would have different units of measurements such as age vs income.\n",
    "\n",
    "    2. Given kmeans iterative nature and the random initialization of centroids at the start of the algorithm, different initializations may lead to different clusters since kmeans algorithm may stuck in a local optimum and may not converge to global optimum. Therefore, it’s recommended to run the algorithm using different initializations of centroids and pick the results of the run that that yielded the lower sum of squared distance.\n",
    "\n",
    "    3. Assignment of examples isn’t changing is the same thing as no change in within-cluster variation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pic4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We’ll use simple implementation of kmeans here to just illustrate some concepts. Then we will use sklearn implementation that is more efficient take care of many things for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class Kmeans:\n",
    "    '''Implementing Kmeans algorithm.'''\n",
    "\n",
    "    def __init__(self, n_clusters, max_iter=100, random_state=123):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def initializ_centroids(self, X):\n",
    "        np.random.RandomState(self.random_state)\n",
    "        random_idx = np.random.permutation(X.shape[0])\n",
    "        centroids = X[random_idx[:self.n_clusters]]\n",
    "        return centroids\n",
    "\n",
    "    def compute_centroids(self, X, labels):\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        for k in range(self.n_clusters):\n",
    "            centroids[k, :] = np.mean(X[labels == k, :], axis=0)\n",
    "        return centroids\n",
    "\n",
    "    def compute_distance(self, X, centroids):\n",
    "        distance = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for k in range(self.n_clusters):\n",
    "            row_norm = norm(X - centroids[k, :], axis=1)\n",
    "            distance[:, k] = np.square(row_norm)\n",
    "        return distance\n",
    "\n",
    "    def find_closest_cluster(self, distance):\n",
    "        return np.argmin(distance, axis=1)\n",
    "\n",
    "    def compute_sse(self, X, labels, centroids):\n",
    "        distance = np.zeros(X.shape[0])\n",
    "        for k in range(self.n_clusters):\n",
    "            distance[labels == k] = norm(X[labels == k] - centroids[k], axis=1)\n",
    "        return np.sum(np.square(distance))\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self.centroids = self.initializ_centroids(X)\n",
    "        for i in range(self.max_iter):\n",
    "            old_centroids = self.centroids\n",
    "            distance = self.compute_distance(X, old_centroids)\n",
    "            self.labels = self.find_closest_cluster(distance)\n",
    "            self.centroids = self.compute_centroids(X, self.labels)\n",
    "            if np.all(old_centroids == self.centroids):\n",
    "                break\n",
    "        self.error = self.compute_sse(X, self.labels, self.centroids)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distance = self.compute_distance(X, old_centroids)\n",
    "        return self.find_closest_cluster(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans on Iris Flower problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This dataset consists of 150 elements of three different types of Iris flower featuring its three main characteristics i.e petal width, petal length and sepal length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!!!!The following code is not being executed in my jupyter notebook. So i have put images of the output. This code works well in spyder.!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chaitu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:67: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Though the following import is not directly being used, it is required\n",
    "# for 3D projection to work\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import datasets\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "estimators = [('k_means_iris_8', KMeans(n_clusters=8)),\n",
    "              ('k_means_iris_3', KMeans(n_clusters=3)),\n",
    "              ('k_means_iris_bad_init', KMeans(n_clusters=3, n_init=1,\n",
    "                                               init='random'))]\n",
    "\n",
    "fignum = 1\n",
    "titles = ['8 clusters', '3 clusters', '3 clusters, bad initialization']\n",
    "for name, est in estimators:\n",
    "    fig = plt.figure(fignum, figsize=(4, 3))\n",
    "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "    est.fit(X)\n",
    "    labels = est.labels_\n",
    "\n",
    "    ax.scatter(X[:, 3], X[:, 0], X[:, 2],\n",
    "               c=labels.astype(np.float), edgecolor='k')\n",
    "\n",
    "    ax.w_xaxis.set_ticklabels([])\n",
    "    ax.w_yaxis.set_ticklabels([])\n",
    "    ax.w_zaxis.set_ticklabels([])\n",
    "    ax.set_xlabel('Petal width')\n",
    "    ax.set_ylabel('Sepal length')\n",
    "    ax.set_zlabel('Petal length')\n",
    "    ax.set_title(titles[fignum - 1])\n",
    "    ax.dist = 12\n",
    "    fignum = fignum + 1\n",
    "\n",
    "# Plot the ground truth\n",
    "fig = plt.figure(fignum, figsize=(4, 3))\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "for name, label in [('Setosa', 0),\n",
    "                    ('Versicolour', 1),\n",
    "                    ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 3].mean(),\n",
    "              X[y == label, 0].mean(),\n",
    "              X[y == label, 2].mean() + 2, name,\n",
    "              horizontalalignment='center',\n",
    "              bbox=dict(alpha=.2, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor='k')\n",
    "\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "ax.set_xlabel('Petal width')\n",
    "ax.set_ylabel('Sepal length')\n",
    "ax.set_zlabel('Petal length')\n",
    "ax.set_title('Ground Truth')\n",
    "ax.dist = 12\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pic5.png)\n",
    "![](pic6.png)\n",
    "![](pic7.png)\n",
    "![](pic8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    So in the above problem i have only shown the 8 cluster part only as an example. Basically this is all about playing around with the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
